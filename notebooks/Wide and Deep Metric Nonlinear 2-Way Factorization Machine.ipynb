{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "goal:\n",
    "=====\n",
    "\n",
    "basically a factorization machine with cross entropy loss where interaction effects come from deep nonlinear relu-activated embeddings and with an additional \"metric\" kernal matrix.\n",
    "\n",
    "todo: dropout. currently no regularization on the interaction layers in the cost function. can handle with FTRL optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import this stuff\n",
    "import time\n",
    "import sys\n",
    "from pylab import *\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embeddings(x, rank, num_features, depth=1, seed=12345):\n",
    "    \"\"\"\n",
    "    assumes that all hidden layers are width `rank`\n",
    "    \"\"\"\n",
    "    assert depth > 0\n",
    "    V = tf.Variable(\n",
    "        tf.random.truncated_normal([rank, num_features], stddev=0.2, mean=0, seed=seed),\n",
    "        name=\"v_1\",\n",
    "    )\n",
    "    b = tf.Variable(\n",
    "        tf.random.truncated_normal([rank, 1], stddev=0.2, mean=0, seed=seed), name=\"b_1\"\n",
    "    )\n",
    "    Vx = tf.nn.relu(tf.matmul(V, x) + b)\n",
    "    for i in range(depth - 1):\n",
    "        V = tf.Variable(\n",
    "            tf.random.truncated_normal([rank, rank], stddev=0.2, mean=0, seed=seed),\n",
    "            name=\"v_%s\" % i,\n",
    "        )\n",
    "        b = tf.Variable(\n",
    "            tf.random.truncated_normal([rank, 1], stddev=0.2, mean=0, seed=seed),\n",
    "            name=\"b_%s\" % i,\n",
    "        )\n",
    "        Vx = tf.nn.relu(tf.matmul(V, Vx) + b)\n",
    "\n",
    "    return Vx\n",
    "\n",
    "\n",
    "def factorize(\n",
    "    observed_features,\n",
    "    labels,\n",
    "    observed_features_validation,\n",
    "    labels_validation,\n",
    "    rank,\n",
    "    max_iter=100,\n",
    "    verbose=False,\n",
    "    lambda_v=0,\n",
    "    lambda_k=0,\n",
    "    lambda_w=0,\n",
    "    lambda_constants=0,\n",
    "    epsilon=0.001,\n",
    "    optimizer=tf.compat.v1.train.AdamOptimizer(),\n",
    "    depth=3,\n",
    "    seed=12345,\n",
    "):\n",
    "\n",
    "    # Extract info about shapes etc from the training data\n",
    "    num_items = observed_features.shape[0]\n",
    "    num_features = observed_features.shape[1]\n",
    "\n",
    "    # matrix defining the inner product weights when doing interactions\n",
    "    K = tf.Variable(\n",
    "        tf.random.truncated_normal([rank, rank], stddev=0.2, mean=0, seed=seed),\n",
    "        name=\"metric_matrix\",\n",
    "    )\n",
    "\n",
    "    # coefficients for linear function on inputs (wide part)\n",
    "    w = tf.Variable(\n",
    "        tf.random.truncated_normal([1, num_features], stddev=0.2, mean=0, seed=seed),\n",
    "        name=\"hyperplane\",\n",
    "    )\n",
    "\n",
    "    # coefficients for linear functinos on inputs (deep part)\n",
    "    lw = tf.Variable(\n",
    "        tf.random.truncated_normal([1, rank], stddev=0.2, mean=0, seed=seed),\n",
    "        name=\"latenthyperplane\",\n",
    "    )\n",
    "\n",
    "    # bias in linear function\n",
    "    b = tf.Variable(\n",
    "        tf.random.truncated_normal([1, 1], stddev=0.2, mean=0, seed=seed), name=\"b_one\"\n",
    "    )\n",
    "\n",
    "    x = tf.placeholder(tf.float32, [None, num_features])\n",
    "    y = tf.placeholder(tf.float32)\n",
    "\n",
    "    norm_x = tf.nn.l2_normalize(x, dim=0)\n",
    "\n",
    "    Vx = make_embeddings(\n",
    "        tf.transpose(norm_x), rank, num_features, depth=depth, seed=seed\n",
    "    )\n",
    "    right_kern = tf.matmul(K, Vx)\n",
    "\n",
    "    full_kern = tf.matmul(tf.transpose(Vx), right_kern)\n",
    "    linear = tf.matmul(w, tf.transpose(norm_x))\n",
    "    latent_linear = tf.matmul(lw, Vx)\n",
    "\n",
    "    pred = tf.reduce_sum(tf.sigmoid(linear + latent_linear + full_kern + b))\n",
    "\n",
    "    # todo: dropout. currently no regularization on the interaction layers in the cost functino\n",
    "    # can handle with FTRL optimization\n",
    "    cost = tf.reduce_mean(\n",
    "        -y * tf.log(pred + 0.0000000001)\n",
    "        - (1 - y) * tf.log((1 - pred + 0.0000000001))\n",
    "        + lambda_k * tf.nn.l2_loss(K)\n",
    "        + lambda_w * tf.nn.l2_loss(w)\n",
    "        + lambda_constants * tf.nn.l2_loss(b)\n",
    "    )\n",
    "    optimize = optimizer.minimize(cost)\n",
    "    norm = tf.reduce_mean(tf.nn.l2_loss(w))\n",
    "\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        last_cost = 1000000\n",
    "        for iter in range(0, max_iter):\n",
    "            avg_cost = 0\n",
    "\n",
    "            for i in range(num_items):\n",
    "                _, c, n = sess.run(\n",
    "                    [optimize, cost, norm],\n",
    "                    feed_dict={\n",
    "                        x: observed_features[i].reshape(1, num_features),\n",
    "                        y: labels[i],\n",
    "                    },\n",
    "                )\n",
    "                avg_cost += c / num_items\n",
    "            if verbose:\n",
    "                print(\"epoch: %s, cost: %s\" % (iter + 1, avg_cost))\n",
    "\n",
    "            # check for convergence\n",
    "            if abs(avg_cost - last_cost) / avg_cost < epsilon:\n",
    "                break\n",
    "\n",
    "            last_cost = avg_cost\n",
    "\n",
    "        if verbose:\n",
    "            print(\"optimization finished\")\n",
    "        predictions = []\n",
    "        total_costs = 0\n",
    "        for i in range(observed_features_validation.shape[0]):\n",
    "            p, c = sess.run(\n",
    "                [pred, cost],\n",
    "                feed_dict={\n",
    "                    x: observed_features_validation[i].reshape(1, num_features),\n",
    "                    y: labels_validation[i],\n",
    "                },\n",
    "            )\n",
    "            predictions.append(p)\n",
    "            total_costs += c\n",
    "        return (\n",
    "            predictions,\n",
    "            total_costs / observed_features_validation.shape[0],\n",
    "            sess.run([norm]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this data for now\n",
    "\n",
    "categories = [\"alt.atheism\", \"soc.religion.christian\", \"comp.graphics\", \"sci.med\"]\n",
    "\n",
    "X, y = datasets.fetch_20newsgroups(\n",
    "    categories=categories,\n",
    "    shuffle=True,\n",
    "    remove=[\"headers\", \"footers\", \"quotes\"],\n",
    "    return_X_y=True,\n",
    ")\n",
    "y = np.array([1 if y_i == 1 else 0 for y_i in y])\n",
    "tfidf = TfidfVectorizer(decode_error=False, min_df=5)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "X_train = tfidf.fit_transform(X_train).todense()\n",
    "X_test = tfidf.transform(X_test).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-6bd4a8d581b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_costs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_v\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rank: %s, cost: %s, overall AUC: %s, norm: %s\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_costs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"weighted\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-fcf1445fc4d9>\u001b[0m in \u001b[0;36mfactorize\u001b[0;34m(observed_features, labels, observed_features_validation, labels_validation, rank, max_iter, verbose, lambda_v, lambda_k, lambda_w, lambda_constants, epsilon, optimizer, depth, seed)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"b_one\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
     ]
    }
   ],
   "source": [
    "r = 10\n",
    "predictions, test_costs, norm = factorize(\n",
    "    X_train, y_train, X_test, y_test, r, verbose=True, lambda_v=0.1, max_iter=300\n",
    ")\n",
    "print(\"rank: %s, cost: %s, overall AUC: %s, norm: %s\") % (\n",
    "    r,\n",
    "    test_costs,\n",
    "    roc_auc_score(y_test, predictions, average=\"weighted\"),\n",
    "    norm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'factorize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5f4eaa104a0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# with some regularization via the optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_costs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFtrlOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_regularization_strength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rank: %s, cost: %s, overall AUC: %s, norm: %s\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_costs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"weighted\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'factorize' is not defined"
     ]
    }
   ],
   "source": [
    "# with some regularization via the optimizer\n",
    "r = 10\n",
    "predictions, test_costs, norm = factorize(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    r,\n",
    "    verbose=True,\n",
    "    max_iter=30,\n",
    "    optimizer=tf.train.FtrlOptimizer(1.0, l2_regularization_strength=1.0),\n",
    ")\n",
    "print(\"rank: %s, cost: %s, overall AUC: %s, norm: %s\") % (\n",
    "    r,\n",
    "    test_costs,\n",
    "    roc_auc_score(y_test, predictions, average=\"weighted\"),\n",
    "    norm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
